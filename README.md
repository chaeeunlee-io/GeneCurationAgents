# Gene-Disease Curation System

# Geneâ€“Disease Validity Curation Demo

## What this repo shows

This repository contains a **minimal working prototype** of an _LLMâ€‘powered multiâ€‘agent pipeline_ that automates large parts of the [ClinGen](https://clinicalgenome.org/) geneâ€“disease validity curation workflow.

<!-- ![pipeline](gene_disease_curation/assets/pipeline.png)

![criteria](gene_disease_curation/assets/sum_matrix.png) -->

<p align="center">
  <img src="gene_disease_curation/assets/pipeline.png" width="400"/>
</p>

<p align="center">
  <img src="gene_disease_curation/assets/sum_matrix.png" width="400"/>
</p>

In roughly 300Â Python lines we:

1.  **Search PubMed** for a _gene Ã— disease_ query.
2.  **Fetch abstracts** for the topÂ _N_ papers.
3.  **Dispatch four specialist agents** (Variant, Functional, Cohort, Segregation) that independently extract structured evidence from each abstract.
4.  **Aggregate & weight** the evidence to compute perâ€‘category scores and a provisional clinicalâ€‘validity classification ("Limited", "Moderate", "Strong", etc.).

The graph is built with **LangGraph**, so every step is a node; each extractor is a classic _toolâ€‘asâ€‘agent_ ğŸ’¡.

---

## Why gene curation is perfect for an LLM multiâ€‘agent design

| Manual curation | How the agents help |
| --- | --- |
| Dozens of papers must be read, key info copyâ€‘pasted, and scored by hand | Agents run focussed extraction passes in parallel; no human copyâ€‘pasting |
| Evidence types are heterogeneous (variants vs. family segregation vs. functional assays) | Domainâ€‘specific prompts + Pydantic parsers keep the JSON schemas disjoint yet composable |
| Scoring rules are formulaic but fiddly (weights, caps, yearsâ€‘sinceâ€‘firstâ€‘report) | A deterministic `calculate_scores` node turns raw agent output into points with one weight table |
| Curators iterate ("add ClinVar", "reâ€‘query PubMed") | Graph structure makes inserting new nodesâ€”e.g. a \_ClinVarEnricher\_â€”a oneâ€‘liner |

---

## Manual effort saved

A trained curator needs **4Â â€“Â 8Â hours** to bring a single geneâ€“disease pair to â€œclassificationâ€‘readyâ€:

*   literature triage
*   perâ€‘patient variant assessment
*   segregation math
*   crossâ€‘checking public databases
*   narrative summary writing

The demo collapses the triageÂ + evidenceâ€‘abstraction phases to **â‰ˆÂ 1Â minute** of wallâ€‘clock time and \<Â $0.10 in API calls, leaving the curator to _review_ rather than _transcribe_.

---

## Code / component map

| Path | Responsibility |
| --- | --- |
| `pubmed.py` | Async PubMed search & abstract fetch |
| `agents.py` etc. | Four specialist extractor classes (promptÂ + parserÂ + `analyze`) |
| `orchestrator.py` | Fanâ€‘out / gather helper that runs the specialists per PMID |
| `/graph` | Builds the StateGraph (`search â†’ fetch â†’ extract â†’ score â†’ classify`) |
| `run_curation.py` | CLI entryâ€‘point â€“ `python run_curation.py BRCA1 "breast cancer"` |

---

## Extending the demo

1.  **Router node** â€“ let an LLM decide which evidence agents to run per abstract.
2.  **Enrichment nodes** â€“ call ClinVar & gnomAD to annotate extracted variants before scoring.
3.  **Critic loop** â€“ add a validator agent that asks a second LLM to sanityâ€‘check each JSON payload.
4.  **Vector memory** â€“ store perâ€‘abstract summaries in a local Chroma DB for crossâ€‘gene reuse.

Each of these is a new LangGraph node plus \<Â 50Â LOC.

---

## Quickâ€‘start

